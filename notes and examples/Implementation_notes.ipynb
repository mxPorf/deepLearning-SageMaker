{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb20a80",
   "metadata": {},
   "source": [
    "## References and notes\n",
    "This file contains reference code and comments on how to implement some of the tasks of this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f37dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "def train(model, train_loader, cost, optimizer, epoch):\n",
    "    model.train()\n",
    "    for e in range(epoch):\n",
    "        running_loss=0\n",
    "        correct=0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            #NOTE: Notice how we are not changing the data shape here\n",
    "            # This is because CNNs expects a 3 dimensional input\n",
    "            pred = model(data)\n",
    "            loss = cost(pred, target)\n",
    "            running_loss+=loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pred=pred.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        print(f\"Epoch {e}: Loss {running_loss/len(train_loader.dataset)}, Accuracy {100*(correct/len(train_loader.dataset))}%\")\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            #NOTE: Notice how we are not changing the data shape here\n",
    "            # This is because CNNs expects a 3 dimensional input\n",
    "            #data.view(data.shape[0], -1)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    print(f'Test set: Accuracy: {correct}/{len(test_loader.dataset)} = {100*(correct/len(test_loader.dataset))}%)')\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convLayer1 = nn.Conv2d(3,6,5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.convLayer2 = nn.Conv2d(6,16,5)\n",
    "        self.fullLayer1 = nn.Linear(16 * 5 * 5, 255)\n",
    "        self.fullLayer2 = nn.Linear(255, 128)\n",
    "        self.fullLayer3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.convLayer1(x)))\n",
    "        x = self.pool(F.relu(self.convLayer2(x)))\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fullLayer1(x))\n",
    "        x = F.relu(self.fullLayer2(x))\n",
    "        x = self.fullLayer3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "batch_size = 32 \n",
    "epoch=2\n",
    "\n",
    "training_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "testing_transform = transforms.Compose([transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "        download=True, transform=training_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "        download=True, transform=testing_transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "        shuffle=False)\n",
    "\n",
    "model=Model()\n",
    "\n",
    "# criterion= nn.NLLLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "train(model, trainloader, criterion, optimizer, epoch)\n",
    "test(model, testloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30c77c",
   "metadata": {},
   "source": [
    "### Example of sagemaker script mode with hyperparameter tuning\n",
    "\n",
    "Following are two scripts used to tune hyperparameters with Sagemaker.\n",
    "<br>\n",
    "<br>\n",
    "Another example can be found [here](https://github.com/aws/amazon-sagemaker-examples/tree/main/frameworks/pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8bee99",
   "metadata": {},
   "source": [
    "#### Upload script\n",
    "Sample upload script that uses Sagemaker script mode to perform hyperparameter tuning on a custom neural network that is trained to classify images in the CIFAR10 dataset. \n",
    "The building and training of the neural network are performed in another script (cifar.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import gzip \n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-pytorch-cifar\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "local_dir = 'data'\n",
    "CIFAR10.mirrors = [\"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/CIFAR10/\"]\n",
    "CIFAR10(\n",
    "    local_dir,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    )\n",
    ")\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path=\"data\", bucket=bucket, key_prefix=prefix)\n",
    "print(\"input spec (in this case, just an S3 path): {}\".format(inputs))\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"cifar.py\",\n",
    "    role=role,\n",
    "    py_version='py36',\n",
    "    framework_version=\"1.8\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"lr\": ContinuousParameter(0.001, 0.1),\n",
    "    \"batch-size\": CategoricalParameter([32, 64, 128, 256, 512]),\n",
    "    \"epochs\": IntegerParameter(2, 4)\n",
    "}\n",
    "\n",
    "objective_metric_name = \"average test loss\"\n",
    "objective_type = \"Minimize\"\n",
    "metric_definitions = [{\"Name\": \"average test loss\", \"Regex\": \"Test set: Average loss: ([0-9\\\\.]+)\"}]\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs=4,\n",
    "    max_parallel_jobs=2,\n",
    "    objective_type=objective_type,\n",
    ")\n",
    "\n",
    "tuner.fit({\"training\": inputs})\n",
    "\n",
    "predictor = tuner.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")\n",
    "\n",
    "# Query the Endpoint\n",
    "\n",
    "file = 'data/cifar-10-batches-py/data_batch_1'\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "data=unpickle(file)\n",
    "data=np.reshape(data[b'data'][0], (3, 32, 32))\n",
    "\n",
    "response = # TODO: Query the endpoint\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9bb2c0",
   "metadata": {},
   "source": [
    "#### cifar.py\n",
    "This is a training script that acoompanies the previous upload script. This file is used in sagemaker to perform training with automatic parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ce7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def _get_train_data_loader(batch_size, training_dir):\n",
    "    logger.info(\"Get train data loader\")\n",
    "    dataset = datasets.CIFAR10(\n",
    "        training_dir,\n",
    "        train=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor()]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "\n",
    "def _get_test_data_loader(test_batch_size, training_dir):\n",
    "    logger.info(\"Get test data loader\")\n",
    "    return torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(\n",
    "            training_dir,\n",
    "            train=False,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor()]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "def train(args):\n",
    "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir)\n",
    "    test_loader = _get_test_data_loader(args.test_batch_size, args.data_dir)\n",
    "\n",
    "    model = Net()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                logger.info(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "        test(model, test_loader)\n",
    "    save_model(model, args.model_dir)\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    logger.info(\n",
    "        \"Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = Net()\n",
    "    with open(os.path.join(model_dir, \"model.pth\"), \"rb\") as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(model, model_dir):\n",
    "    logger.info(\"Saving the model.\")\n",
    "    path = os.path.join(model_dir, \"model.pth\")\n",
    "    torch.save(model.cpu().state_dict(), path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data and model checkpoints directories\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for training (default: 64)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test-batch-size\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for testing (default: 1000)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "        metavar=\"N\",\n",
    "        help=\"number of epochs to train (default: 10)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\", type=float, default=0.01, metavar=\"LR\", help=\"learning rate (default: 0.01)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--momentum\", type=float, default=0.5, metavar=\"M\", help=\"SGD momentum (default: 0.5)\"\n",
    "    )\n",
    "\n",
    "    # Container environment\n",
    "    parser.add_argument(\"--hosts\", type=list, default=json.loads(os.environ[\"SM_HOSTS\"]))\n",
    "    parser.add_argument(\"--current-host\", type=str, default=os.environ[\"SM_CURRENT_HOST\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--data-dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAINING\"])\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "\n",
    "    train(parser.parse_args())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df884a45",
   "metadata": {},
   "source": [
    "### Example debugger and profiler scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ae1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import Rule, ProfilerRule, rule_configs\n",
    "from sagemaker.debugger import DebuggerHookConfig, ProfilerConfig, FrameworkProfile\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import boto3\n",
    "from smdebug.trials import create_trial\n",
    "from smdebug.core.modes import ModeKeys\n",
    "from smdebug.profiler.analysis.notebook_utils.training_job import TrainingJob\n",
    "from smdebug.profiler.analysis.notebook_utils.timeline_charts import TimelineCharts\n",
    "import os\n",
    "import IPython\n",
    "\n",
    "rules = [\n",
    "    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "    ProfilerRule.sagemaker(rule_configs.LowGPUUtilization()),\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "    Rule.sagemaker(rule_configs.vanishing_gradient()),\n",
    "    Rule.sagemaker(rule_configs.overfit()),\n",
    "    Rule.sagemaker(rule_configs.overtraining()),\n",
    "    Rule.sagemaker(rule_configs.poor_weight_initialization()),\n",
    "]\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=500, framework_profile_params=FrameworkProfile(num_steps=10)\n",
    ")\n",
    "debugger_config = DebuggerHookConfig(\n",
    "    hook_parameters={\"train.save_interval\": \"100\", \"eval.save_interval\": \"10\"}\n",
    ")\n",
    "\n",
    "estimator = PyTorch(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    source_dir=\"scripts\",\n",
    "    entry_point=\"pytorch_cifar_profiling.py\",\n",
    "    framework_version=\"1.8\",\n",
    "    py_version=\"py36\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    profiler_config=profiler_config,\n",
    "    debugger_hook_config=debugger_config,\n",
    "    rules=rules,\n",
    ")\n",
    "\n",
    "estimator.fit(wait=True)\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "print(f\"Training jobname: {training_job_name}\")\n",
    "print(f\"Region: {region}\")\n",
    "\n",
    "trial = create_trial(estimator.latest_job_debugger_artifacts_path())\n",
    "\n",
    "print(trial.tensor_names())\n",
    "print(len(trial.tensor(\"CrossEntropyLoss_output_0\").steps(mode=ModeKeys.TRAIN)))\n",
    "print(len(trial.tensor(\"CrossEntropyLoss_output_0\").steps(mode=ModeKeys.EVAL)))\n",
    "\n",
    "tj = TrainingJob(training_job_name, region)\n",
    "tj.wait_for_sys_profiling_data_to_be_available()\n",
    "\n",
    "system_metrics_reader = tj.get_systems_metrics_reader()\n",
    "system_metrics_reader.refresh_event_file_list()\n",
    "\n",
    "view_timeline_charts = TimelineCharts(\n",
    "    system_metrics_reader,\n",
    "    framework_metrics_reader=None,\n",
    "    select_dimensions=[\"CPU\", \"GPU\"],\n",
    "    select_events=[\"total\"],\n",
    ")\n",
    "\n",
    "rule_output_path = estimator.output_path + estimator.latest_training_job.job_name + \"/rule-output\"\n",
    "print(f\"You will find the profiler report in {rule_output_path}\")\n",
    "\n",
    "! aws s3 ls {rule_output_path} --recursive\n",
    "! aws s3 cp {rule_output_path} ./ --recursive\n",
    "\n",
    "# get the autogenerated folder name of profiler report\n",
    "profiler_report_name = [\n",
    "    rule[\"RuleConfigurationName\"]\n",
    "    for rule in estimator.latest_training_job.rule_job_summary()\n",
    "    if \"Profiler\" in rule[\"RuleConfigurationName\"]\n",
    "][0]\n",
    "\n",
    "IPython.display.HTML(filename=profiler_report_name + \"/profiler-output/profiler-report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468fcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb9084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from smdebug import modes\n",
    "from smdebug.profiler.utils import str2bool\n",
    "from smdebug.pytorch import get_hook\n",
    "\n",
    "def train(args, net, device):\n",
    "    hook = get_hook(create_if_not_exists=True)\n",
    "    batch_size = args.batch_size\n",
    "    epoch = args.epoch\n",
    "    transform_train = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    transform_valid = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    validset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=False, download=True, transform=transform_valid\n",
    "    )\n",
    "    validloader = torch.utils.data.DataLoader(\n",
    "        validset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    loss_optim = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=1.0, momentum=0.9)\n",
    "\n",
    "    epoch_times = []\n",
    "\n",
    "    if hook:\n",
    "        hook.register_loss(loss_optim)\n",
    "    # train the model\n",
    "\n",
    "    for i in range(epoch):\n",
    "        print(\"START TRAINING\")\n",
    "        if hook:\n",
    "            hook.set_mode(modes.TRAIN)\n",
    "        start = time.time()\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        for _, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_optim(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(\"START VALIDATING\")\n",
    "        if hook:\n",
    "            hook.set_mode(modes.EVAL)\n",
    "        net.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for _, (inputs, targets) in enumerate(validloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = loss_optim(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        epoch_time = time.time() - start\n",
    "        epoch_times.append(epoch_time)\n",
    "        print(\n",
    "            \"Epoch %d: train loss %.3f, val loss %.3f, in %.1f sec\"\n",
    "            % (i, train_loss, val_loss, epoch_time)\n",
    "        )\n",
    "\n",
    "    # calculate training time after all epoch\n",
    "    p50 = np.percentile(epoch_times, 50)\n",
    "    return p50\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--epoch\", type=int, default=1)\n",
    "    parser.add_argument(\"--gpu\", type=str2bool, default=True)\n",
    "    parser.add_argument(\"--model\", type=str, default=\"resnet50\")\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    for key, value in vars(opt).items():\n",
    "        print(f\"{key}:{value}\")\n",
    "    # create model\n",
    "    net = models.__dict__[opt.model](pretrained=True)\n",
    "    if opt.gpu == 1:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    net.to(device)\n",
    "\n",
    "    # Start the training.\n",
    "    median_time = train(opt, net, device)\n",
    "    print(\"Median training time per Epoch=%.1f sec\" % median_time)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f115f367",
   "metadata": {},
   "source": [
    "#### Script mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc01145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "hyperparameters = {\"epochs\": \"2\", \"batch-size\": \"32\", \"test-batch-size\": \"100\", \"lr\": \"0.001\"}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"scripts/pytorch_cifar.py\",\n",
    "    base_job_name=\"sagemaker-script-mode\",\n",
    "    role=get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    framework_version=\"1.8\",\n",
    "    py_version=\"py36\",\n",
    ")\n",
    "\n",
    "estimator.fit(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d37c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for training (default: 64)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test-batch-size\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for testing (default: 1000)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=14,\n",
    "        metavar=\"N\",\n",
    "        help=\"number of epochs to train (default: 14)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\", type=float, default=1.0, metavar=\"LR\", help=\"learning rate (default: 1.0)\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train_kwargs = {\"batch_size\": args.batch_size}\n",
    "    test_kwargs = {\"batch_size\": args.test_batch_size}\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    )\n",
    "    dataset1 = datasets.CIFAR10(\"../data\", train=True, download=True, transform=transform)\n",
    "    dataset2 = datasets.CIFAR10(\"../data\", train=False, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net()\n",
    "\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(model, train_loader, optimizer, epoch)\n",
    "        test(model, test_loader)\n",
    "\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
