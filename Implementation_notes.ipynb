{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb20a80",
   "metadata": {},
   "source": [
    "## References and notes\n",
    "This file contains reference code and comments on how to implement some of the tasks of this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f37dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "def train(model, train_loader, cost, optimizer, epoch):\n",
    "    model.train()\n",
    "    for e in range(epoch):\n",
    "        running_loss=0\n",
    "        correct=0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            #NOTE: Notice how we are not changing the data shape here\n",
    "            # This is because CNNs expects a 3 dimensional input\n",
    "            pred = model(data)\n",
    "            loss = cost(pred, target)\n",
    "            running_loss+=loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pred=pred.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        print(f\"Epoch {e}: Loss {running_loss/len(train_loader.dataset)}, Accuracy {100*(correct/len(train_loader.dataset))}%\")\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            #NOTE: Notice how we are not changing the data shape here\n",
    "            # This is because CNNs expects a 3 dimensional input\n",
    "            #data.view(data.shape[0], -1)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    print(f'Test set: Accuracy: {correct}/{len(test_loader.dataset)} = {100*(correct/len(test_loader.dataset))}%)')\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convLayer1 = nn.Conv2d(3,6,5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.convLayer2 = nn.Conv2d(6,16,5)\n",
    "        self.fullLayer1 = nn.Linear(16 * 5 * 5, 255)\n",
    "        self.fullLayer2 = nn.Linear(255, 128)\n",
    "        self.fullLayer3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.convLayer1(x)))\n",
    "        x = self.pool(F.relu(self.convLayer2(x)))\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fullLayer1(x))\n",
    "        x = F.relu(self.fullLayer2(x))\n",
    "        x = self.fullLayer3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "batch_size = 32 \n",
    "epoch=2\n",
    "\n",
    "training_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "testing_transform = transforms.Compose([transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "        download=True, transform=training_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "        download=True, transform=testing_transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "        shuffle=False)\n",
    "\n",
    "model=Model()\n",
    "\n",
    "# criterion= nn.NLLLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "train(model, trainloader, criterion, optimizer, epoch)\n",
    "test(model, testloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30c77c",
   "metadata": {},
   "source": [
    "### Example of sagemaker script mode with hyperparameter tuning\n",
    "\n",
    "Following are two scripts used to tune hyperparameters with Sagemaker.\n",
    "<br>\n",
    "<br>\n",
    "Another example can be found [here](https://github.com/aws/amazon-sagemaker-examples/tree/main/frameworks/pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8bee99",
   "metadata": {},
   "source": [
    "#### Upload script\n",
    "Sample upload script that uses Sagemaker script mode to perform hyperparameter tuning on a custom neural network that is trained to classify images in the CIFAR10 dataset. \n",
    "The building and training of the neural network are performed in another script (cifar.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import gzip \n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-pytorch-cifar\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "local_dir = 'data'\n",
    "CIFAR10.mirrors = [\"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/CIFAR10/\"]\n",
    "CIFAR10(\n",
    "    local_dir,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    )\n",
    ")\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path=\"data\", bucket=bucket, key_prefix=prefix)\n",
    "print(\"input spec (in this case, just an S3 path): {}\".format(inputs))\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"cifar.py\",\n",
    "    role=role,\n",
    "    py_version='py36',\n",
    "    framework_version=\"1.8\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"lr\": ContinuousParameter(0.001, 0.1),\n",
    "    \"batch-size\": CategoricalParameter([32, 64, 128, 256, 512]),\n",
    "    \"epochs\": IntegerParameter(2, 4)\n",
    "}\n",
    "\n",
    "objective_metric_name = \"average test loss\"\n",
    "objective_type = \"Minimize\"\n",
    "metric_definitions = [{\"Name\": \"average test loss\", \"Regex\": \"Test set: Average loss: ([0-9\\\\.]+)\"}]\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs=4,\n",
    "    max_parallel_jobs=2,\n",
    "    objective_type=objective_type,\n",
    ")\n",
    "\n",
    "tuner.fit({\"training\": inputs})\n",
    "\n",
    "predictor = tuner.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")\n",
    "\n",
    "# Query the Endpoint\n",
    "\n",
    "file = 'data/cifar-10-batches-py/data_batch_1'\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "data=unpickle(file)\n",
    "data=np.reshape(data[b'data'][0], (3, 32, 32))\n",
    "\n",
    "response = # TODO: Query the endpoint\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9bb2c0",
   "metadata": {},
   "source": [
    "#### cifar.py\n",
    "This is a training script that acoompanies the previous upload script. This file is used in sagemaker to perform training with automatic parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ce7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def _get_train_data_loader(batch_size, training_dir):\n",
    "    logger.info(\"Get train data loader\")\n",
    "    dataset = datasets.CIFAR10(\n",
    "        training_dir,\n",
    "        train=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor()]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "\n",
    "def _get_test_data_loader(test_batch_size, training_dir):\n",
    "    logger.info(\"Get test data loader\")\n",
    "    return torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(\n",
    "            training_dir,\n",
    "            train=False,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor()]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "def train(args):\n",
    "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir)\n",
    "    test_loader = _get_test_data_loader(args.test_batch_size, args.data_dir)\n",
    "\n",
    "    model = Net()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                logger.info(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "        test(model, test_loader)\n",
    "    save_model(model, args.model_dir)\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    logger.info(\n",
    "        \"Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = Net()\n",
    "    with open(os.path.join(model_dir, \"model.pth\"), \"rb\") as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(model, model_dir):\n",
    "    logger.info(\"Saving the model.\")\n",
    "    path = os.path.join(model_dir, \"model.pth\")\n",
    "    torch.save(model.cpu().state_dict(), path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data and model checkpoints directories\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for training (default: 64)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test-batch-size\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for testing (default: 1000)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "        metavar=\"N\",\n",
    "        help=\"number of epochs to train (default: 10)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\", type=float, default=0.01, metavar=\"LR\", help=\"learning rate (default: 0.01)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--momentum\", type=float, default=0.5, metavar=\"M\", help=\"SGD momentum (default: 0.5)\"\n",
    "    )\n",
    "\n",
    "    # Container environment\n",
    "    parser.add_argument(\"--hosts\", type=list, default=json.loads(os.environ[\"SM_HOSTS\"]))\n",
    "    parser.add_argument(\"--current-host\", type=str, default=os.environ[\"SM_CURRENT_HOST\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--data-dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAINING\"])\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "\n",
    "    train(parser.parse_args())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
